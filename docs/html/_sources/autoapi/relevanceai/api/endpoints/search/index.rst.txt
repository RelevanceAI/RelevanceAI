:py:mod:`relevanceai.api.endpoints.search`
==========================================

.. py:module:: relevanceai.api.endpoints.search


Module Contents
---------------

Classes
~~~~~~~

.. autoapisummary::

   relevanceai.api.endpoints.search.SearchClient




.. py:class:: SearchClient(project, api_key)

   Bases: :py:obj:`relevanceai.base._Base`

   Base class for all relevanceai client utilities

   .. py:method:: vector(self, dataset_id: str, multivector_query: List, positive_document_ids: dict = {}, negative_document_ids: dict = {}, vector_operation='sum', approximation_depth=0, sum_fields=True, page_size=20, page=1, similarity_metric='cosine', facets=[], filters=[], min_score=0, select_fields=[], include_vector=False, include_count=True, asc=False, keep_search_history=False, hundred_scale=False, search_history_id=None, query: str = None)

      Allows you to leverage vector similarity search to create a semantic search engine. Powerful features of VecDB vector search:

      1. Multivector search that allows you to search with multiple vectors and give each vector a different weight. e.g. Search with a product image vector and text description vector to find the most similar products by what it looks like and what its described to do. You can also give weightings of each vector field towards the search, e.g. image_vector_ weights 100%, whilst description_vector_ 50%

          An example of a simple multivector query:

          >>> [
          >>>     {"vector": [0.12, 0.23, 0.34], "fields": ["name_vector_"], "alias":"text"},
          >>>     {"vector": [0.45, 0.56, 0.67], "fields": ["image_vector_"], "alias":"image"},
          >>> ]

          An example of a weighted multivector query:

          >>> [
          >>>     {"vector": [0.12, 0.23, 0.34], "fields": {"name_vector_":0.6}, "alias":"text"},
          >>>     {"vector": [0.45, 0.56, 0.67], "fields": {"image_vector_"0.4}, "alias":"image"},
          >>> ]

          An example of a weighted multivector query with multiple fields for each vector:

          >>> [
          >>>     {"vector": [0.12, 0.23, 0.34], "fields": {"name_vector_":0.6, "description_vector_":0.3}, "alias":"text"},
          >>>     {"vector": [0.45, 0.56, 0.67], "fields": {"image_vector_"0.4}, "alias":"image"},
          >>> ]

      2. Utilise faceted search with vector search. For information on how to apply facets/filters check out datasets.documents.get_where

      3. Sum Fields option to adjust whether you want multiple vectors to be combined in the scoring or compared in the scoring. e.g. image_vector_ + text_vector_ or image_vector_ vs text_vector_.

          When sum_fields=True:

          - Multi-vector search allows you to obtain search scores by taking the sum of these scores.
          - TextSearchScore + ImageSearchScore = SearchScore
          - We then rank by the new SearchScore, so for searching 1000 documents there will be 1000 search scores and results

          When sum_fields=False:

          - Multi vector search but not summing the score, instead including it in the comparison!
          - TextSearchScore = SearchScore1
          - ImagSearchScore = SearchScore2
          - We then rank by the 2 new SearchScore, so for searching 1000 documents there should be 2000 search scores and results.

      4. Personalization with positive and negative document ids.

          - For more information about the positive and negative document ids to personalize check out services.recommend.vector

      For more even more advanced configuration and customisation of vector search, reach out to us at dev@relevance.ai and learn about our new advanced_vector_search.

      :param dataset_id: Unique name of dataset
      :type dataset_id: string
      :param multivector_query: Query for advance search that allows for multiple vector and field querying.
      :type multivector_query: list
      :param positive_document_ids: Positive document IDs to personalize the results with, this will retrive the vectors from the document IDs and consider it in the operation.
      :type positive_document_ids: dict
      :param negative_document_ids: Negative document IDs to personalize the results with, this will retrive the vectors from the document IDs and consider it in the operation.
      :type negative_document_ids: dict
      :param approximation_depth: Used for approximate search to speed up search. The higher the number, faster the search but potentially less accurate.
      :type approximation_depth: int
      :param vector_operation: Aggregation for the vectors when using positive and negative document IDs, choose from ['mean', 'sum', 'min', 'max', 'divide', 'mulitple']
      :type vector_operation: string
      :param sum_fields: Whether to sum the multiple vectors similarity search score as 1 or seperate
      :type sum_fields: bool
      :param page_size: Size of each page of results
      :type page_size: int
      :param page: Page of the results
      :type page: int
      :param similarity_metric: Similarity Metric, choose from ['cosine', 'l1', 'l2', 'dp']
      :type similarity_metric: string
      :param facets: Fields to include in the facets, if [] then all
      :type facets: list
      :param filters: Query for filtering the search results
      :type filters: list
      :param min_score: Minimum score for similarity metric
      :type min_score: int
      :param select_fields: Fields to include in the search results, empty array/list means all fields.
      :type select_fields: list
      :param include_vector: Include vectors in the search results
      :type include_vector: bool
      :param include_count: Include the total count of results in the search results
      :type include_count: bool
      :param asc: Whether to sort results by ascending or descending order
      :type asc: bool
      :param keep_search_history: Whether to store the history into VecDB. This will increase the storage costs over time.
      :type keep_search_history: bool
      :param hundred_scale: Whether to scale up the metric by 100
      :type hundred_scale: bool
      :param search_history_id: Search history ID, only used for storing search histories.
      :type search_history_id: string
      :param query: What to store as the query name in the dashboard
      :type query: string


   .. py:method:: hybrid(self, dataset_id: str, multivector_query: List, text: str, fields: list, edit_distance: int = -1, ignore_spaces: bool = True, traditional_weight: float = 0.075, page_size: int = 20, page=1, similarity_metric='cosine', facets=[], filters=[], min_score=0, select_fields=[], include_vector=False, include_count=True, asc=False, keep_search_history=False, hundred_scale=False, search_history_id=None)

      Combine the best of both traditional keyword faceted search with semantic vector search to create the best search possible.


      For information on how to use vector search check out services.search.vector.


      For information on how to use traditional keyword faceted search check out services.search.traditional.

      :param dataset_id: Unique name of dataset
      :type dataset_id: string
      :param multivector_query: Query for advance search that allows for multiple vector and field querying.
      :type multivector_query: list
      :param text: Text Search Query (not encoded as vector)
      :type text: string
      :param fields: Text fields to search against
      :type fields: list
      :param positive_document_ids: Positive document IDs to personalize the results with, this will retrive the vectors from the document IDs and consider it in the operation.
      :type positive_document_ids: dict
      :param negative_document_ids: Negative document IDs to personalize the results with, this will retrive the vectors from the document IDs and consider it in the operation.
      :type negative_document_ids: dict
      :param approximation_depth: Used for approximate search to speed up search. The higher the number, faster the search but potentially less accurate.
      :type approximation_depth: int
      :param vector_operation: Aggregation for the vectors when using positive and negative document IDs, choose from ['mean', 'sum', 'min', 'max', 'divide', 'mulitple']
      :type vector_operation: string
      :param sum_fields: Whether to sum the multiple vectors similarity search score as 1 or seperate
      :type sum_fields: bool
      :param page_size: Size of each page of results
      :type page_size: int
      :param page: Page of the results
      :type page: int
      :param similarity_metric: Similarity Metric, choose from ['cosine', 'l1', 'l2', 'dp']
      :type similarity_metric: string
      :param facets: Fields to include in the facets, if [] then all
      :type facets: list
      :param filters: Query for filtering the search results
      :type filters: list
      :param min_score: Minimum score for similarity metric
      :type min_score: float
      :param select_fields: Fields to include in the search results, empty array/list means all fields.
      :type select_fields: list
      :param include_vector: Include vectors in the search results
      :type include_vector: bool
      :param include_count: Include the total count of results in the search results
      :type include_count: bool
      :param asc: Whether to sort results by ascending or descending order
      :type asc: bool
      :param keep_search_history: Whether to store the history into VecDB. This will increase the storage costs over time.
      :type keep_search_history: bool
      :param hundred_scale: Whether to scale up the metric by 100
      :type hundred_scale: bool
      :param search_history_id: Search history ID, only used for storing search histories.
      :type search_history_id: string
      :param edit_distance: This refers to the amount of letters it takes to reach from 1 string to another string. e.g. band vs bant is a 1 word edit distance. Use -1 if you would like this to be automated.
      :type edit_distance: int
      :param ignore_spaces: Whether to consider cases when there is a space in the word. E.g. Go Pro vs GoPro.
      :type ignore_spaces: bool
      :param traditional_weight: Multiplier of traditional search score. A value of 0.025~0.075 is the ideal range
      :type traditional_weight: int


   .. py:method:: semantic(self, dataset_id: str, multivector_query: list, fields: list, text: str, page_size: int = 20, page=1, similarity_metric='cosine', facets=[], filters=[], min_score=0, select_fields=[], include_vector=False, include_count=True, asc=False, keep_search_history=False, hundred_scale=False)

      A more automated hybrid search with a few extra things that automatically adjusts some of the key parameters for more automated and good out of the box results.


      For information on how to configure semantic search check out services.search.hybrid.

      :param dataset_id: Unique name of dataset
      :type dataset_id: string
      :param multivector_query: Query for advance search that allows for multiple vector and field querying.
      :type multivector_query: list
      :param positive_document_ids: Positive document IDs to personalize the results with, this will retrive the vectors from the document IDs and consider it in the operation.
      :type positive_document_ids: dict
      :param negative_document_ids: Negative document IDs to personalize the results with, this will retrive the vectors from the document IDs and consider it in the operation.
      :type negative_document_ids: dict
      :param text: Text Search Query (not encoded as vector)
      :type text: string
      :param fields: Text fields to search against
      :type fields: list
      :param approximation_depth: Used for approximate search to speed up search. The higher the number, faster the search but potentially less accurate.
      :type approximation_depth: int
      :param sum_fields: Whether to sum the multiple vectors similarity search score as 1 or seperate
      :type sum_fields: bool
      :param page_size: Size of each page of results
      :type page_size: int
      :param page: Page of the results
      :type page: int
      :param similarity_metric: Similarity Metric, choose from ['cosine', 'l1', 'l2', 'dp']
      :type similarity_metric: string
      :param facets: Fields to include in the facets, if [] then all
      :type facets: list
      :param filters: Query for filtering the search results
      :type filters: list
      :param min_score: Minimum score for similarity metric
      :type min_score: int
      :param select_fields: Fields to include in the search results, empty array/list means all fields.
      :type select_fields: list
      :param include_vector: Include vectors in the search results
      :type include_vector: bool
      :param include_count: Include the total count of results in the search results
      :type include_count: bool
      :param asc: Whether to sort results by ascending or descending order
      :type asc: bool
      :param keep_search_history: Whether to store the history into VecDB. This will increase the storage costs over time.
      :type keep_search_history: bool
      :param hundred_scale: Whether to scale up the metric by 100
      :type hundred_scale: bool


   .. py:method:: diversity(self, dataset_id: str, cluster_vector_field: str, n_clusters: int, multivector_query: list, positive_document_ids: dict = {}, negative_document_ids: dict = {}, vector_operation='sum', approximation_depth: int = 0, sum_fields: bool = True, page_size: int = 20, page: int = 1, similarity_metric='cosine', facets=[], filters=[], min_score=0, select_fields=[], include_vector=False, include_count=True, asc=False, keep_search_history=False, hundred_scale: bool = False, search_history_id: str = None, n_init: int = 5, n_iter: int = 10, return_as_clusters: bool = False, query: str = None)

      This will first perform an advanced search and then cluster the top X (page_size) search results. Results are returned as such: Once you have the clusters:

      >>> Cluster 0: [A, B, C]
      >>> Cluster 1: [D, E]
      >>> Cluster 2: [F, G]
      >>> Cluster 3: [H, I]

      (Note, each cluster is ordered by highest to lowest search score.)


      This intermediately returns:

      >>> results_batch_1: [A, H, F, D] (ordered by highest search score)
      >>> results_batch_2: [G, E, B, I] (ordered by highest search score)
      >>> results_batch_3: [C]

      This then returns the final results:

      >>> results: [A, H, F, D, G, E, B, I, C]

      :param dataset_id: Unique name of dataset
      :type dataset_id: string
      :param cluster_vector_field: The field to cluster on.
      :type cluster_vector_field: str
      :param multivector_query: Query for advance search that allows for multiple vector and field querying.
      :type multivector_query: list
      :param positive_document_ids: Positive document IDs to personalize the results with, this will retrive the vectors from the document IDs and consider it in the operation.
      :type positive_document_ids: dict
      :param negative_document_ids: Negative document IDs to personalize the results with, this will retrive the vectors from the document IDs and consider it in the operation.
      :type negative_document_ids: dict
      :param approximation_depth: Used for approximate search to speed up search. The higher the number, faster the search but potentially less accurate.
      :type approximation_depth: int
      :param vector_operation: Aggregation for the vectors when using positive and negative document IDs, choose from ['mean', 'sum', 'min', 'max', 'divide', 'mulitple']
      :type vector_operation: string
      :param sum_fields: Whether to sum the multiple vectors similarity search score as 1 or seperate
      :type sum_fields: bool
      :param page_size: Size of each page of results
      :type page_size: int
      :param page: Page of the results
      :type page: int
      :param similarity_metric: Similarity Metric, choose from ['cosine', 'l1', 'l2', 'dp']
      :type similarity_metric: string
      :param facets: Fields to include in the facets, if [] then all
      :type facets: list
      :param filters: Query for filtering the search results
      :type filters: list
      :param min_score: Minimum score for similarity metric
      :type min_score: int
      :param select_fields: Fields to include in the search results, empty array/list means all fields.
      :type select_fields: list
      :param include_vector: Include vectors in the search results
      :type include_vector: bool
      :param include_count: Include the total count of results in the search results
      :type include_count: bool
      :param asc: Whether to sort results by ascending or descending order
      :type asc: bool
      :param keep_search_history: Whether to store the history into VecDB. This will increase the storage costs over time.
      :type keep_search_history: bool
      :param hundred_scale: Whether to scale up the metric by 100
      :type hundred_scale: bool
      :param search_history_id: Search history ID, only used for storing search histories.
      :type search_history_id: str
      :param n_clusters: Number of clusters to be specified.
      :type n_clusters: int
      :param n_init: Number of runs to run with different centroid seeds
      :type n_init: int
      :param n_iter: Number of iterations in each run
      :type n_iter: int
      :param return_as_clusters: If True, return as clusters as opposed to results list
      :type return_as_clusters: bool
      :param query: What to store as the query name in the dashboard
      :type query: string


   .. py:method:: traditional(self, dataset_id: str, text: str, fields: list = [], edit_distance: int = -1, ignore_spaces: bool = True, page_size: int = 29, page: int = 1, select_fields: list = [], include_vector: bool = False, include_count: bool = True, asc: bool = False, keep_search_history: bool = False, search_history_id: str = None)

      Traditional Faceted Keyword Search with edit distance/fuzzy matching.


      For information on how to apply facets/filters check out datasets.documents.get_where.


      For information on how to construct the facets section for your search bar check out datasets.facets.

      :param dataset_id: Unique name of dataset
      :type dataset_id: string
      :param multivector_query: Query for advance search that allows for multiple vector and field querying.
      :type multivector_query: list
      :param text: Text Search Query (not encoded as vector)
      :type text: string
      :param fields: Text fields to search against
      :type fields: list
      :param edit_distance: This refers to the amount of letters it takes to reach from 1 string to another string. e.g. band vs bant is a 1 word edit distance. Use -1 if you would like this to be automated.
      :type edit_distance: int
      :param ignore_spaces: Whether to consider cases when there is a space in the word. E.g. Go Pro vs GoPro.
      :type ignore_spaces: bool
      :param page_size: Size of each page of results
      :type page_size: int
      :param page: Page of the results
      :type page: int
      :param select_fields: Fields to include in the search results, empty array/list means all fields.
      :type select_fields: list
      :param include_vector: Include vectors in the search results
      :type include_vector: bool
      :param include_count: Include the total count of results in the search results
      :type include_count: bool
      :param asc: Whether to sort results by ascending or descending order
      :type asc: bool
      :param keep_search_history: Whether to store the history into VecDB. This will increase the storage costs over time.
      :type keep_search_history: bool
      :param search_history_id: Search history ID, only used for storing search histories.
      :type search_history_id: string


   .. py:method:: chunk(self, dataset_id, multivector_query, chunk_field, chunk_scoring='max', chunk_page_size: int = 3, chunk_page: int = 1, approximation_depth: int = 0, sum_fields: bool = True, page_size: int = 20, page: int = 1, similarity_metric: str = 'cosine', facets: list = [], filters: list = [], min_score: int = None, include_vector: bool = False, include_count: bool = True, asc: bool = False, keep_search_history: bool = False, hundred_scale: bool = False, query: str = None)

      Chunks are data that has been divided into different units. e.g. A paragraph is made of many sentence chunks, a sentence is made of many word chunks, an image frame in a video. By searching through chunks you can pinpoint more specifically where a match is occuring. When creating a chunk in your document use the suffix "chunk" and "chunkvector". An example of a document with chunks:

      >>> {
      >>>     "_id" : "123",
      >>>     "title" : "Lorem Ipsum Article",
      >>>     "description" : "Lorem Ipsum is simply dummy text of the printing and typesetting industry. Lorem Ipsum has been the industry's standard dummy text ever since the 1500s, when an unknown printer took a galley of type and scrambled it to make a type specimen book. It has survived not only five centuries, but also the leap into electronic typesetting, remaining essentially unchanged.",
      >>>     "description_vector_" : [1.1, 1.2, 1.3],
      >>>     "description_sentence_chunk_" : [
      >>>         {"sentence_id" : 0, "sentence_chunkvector_" : [0.1, 0.2, 0.3], "sentence" : "Lorem Ipsum is simply dummy text of the printing and typesetting industry."},
      >>>         {"sentence_id" : 1, "sentence_chunkvector_" : [0.4, 0.5, 0.6], "sentence" : "Lorem Ipsum has been the industry's standard dummy text ever since the 1500s, when an unknown printer took a galley of type and scrambled it to make a type specimen book."},
      >>>         {"sentence_id" : 2, "sentence_chunkvector_" : [0.7, 0.8, 0.9], "sentence" : "It has survived not only five centuries, but also the leap into electronic typesetting, remaining essentially unchanged."},
      >>>     ]
      >>> }

      For combining chunk search with other search check out services.search.advanced_chunk.

      :param dataset_id: Unique name of dataset
      :type dataset_id: string
      :param multivector_query: Query for advance search that allows for multiple vector and field querying.
      :type multivector_query: list
      :param chunk_field: Field where the array of chunked documents are.
      :type chunk_field: string
      :param chunk_scoring: Scoring method for determining for ranking between document chunks.
      :type chunk_scoring: string
      :param chunk_page_size: Size of each page of chunk results
      :type chunk_page_size: int
      :param chunk_page: Page of the chunk results
      :type chunk_page: int
      :param approximation_depth: Used for approximate search to speed up search. The higher the number, faster the search but potentially less accurate.
      :type approximation_depth: int
      :param sum_fields: Whether to sum the multiple vectors similarity search score as 1 or seperate
      :type sum_fields: bool
      :param page_size: Size of each page of results
      :type page_size: int
      :param page: Page of the results
      :type page: int
      :param similarity_metric: Similarity Metric, choose from ['cosine', 'l1', 'l2', 'dp']
      :type similarity_metric: string
      :param facets: Fields to include in the facets, if [] then all
      :type facets: list
      :param filters: Query for filtering the search results
      :type filters: list
      :param min_score: Minimum score for similarity metric
      :type min_score: int
      :param include_vector: Include vectors in the search results
      :type include_vector: bool
      :param include_count: Include the total count of results in the search results
      :type include_count: bool
      :param asc: Whether to sort results by ascending or descending order
      :type asc: bool
      :param keep_search_history: Whether to store the history into VecDB. This will increase the storage costs over time.
      :type keep_search_history: bool
      :param hundred_scale: Whether to scale up the metric by 100
      :type hundred_scale: bool
      :param query: What to store as the query name in the dashboard
      :type query: string


   .. py:method:: multistep_chunk(self, dataset_id, multivector_query, first_step_multivector_query, chunk_field, chunk_scoring='max', chunk_page_size: int = 3, chunk_page: int = 1, approximation_depth: int = 0, sum_fields: bool = True, page_size: int = 20, page: int = 1, similarity_metric: str = 'cosine', facets: list = [], filters: list = [], min_score: int = None, include_vector: bool = False, include_count: bool = True, asc: bool = False, keep_search_history: bool = False, hundred_scale: bool = False, first_step_page: int = 1, first_step_page_size: int = 20, query: str = None)

      Multistep chunk search involves a vector search followed by chunk search, used to accelerate chunk searches or to identify context before delving into relevant chunks. e.g. Search against the paragraph vector first then sentence chunkvector after.


      For more information about chunk search check out services.search.chunk.


      For more information about vector search check out services.search.vector

      :param dataset_id: Unique name of dataset
      :type dataset_id: string
      :param multivector_query: Query for advance search that allows for multiple vector and field querying.
      :type multivector_query: list
      :param chunk_field: Field where the array of chunked documents are.
      :type chunk_field: string
      :param chunk_scoring: Scoring method for determining for ranking between document chunks.
      :type chunk_scoring: string
      :param chunk_page_size: Size of each page of chunk results
      :type chunk_page_size: int
      :param chunk_page: Page of the chunk results
      :type chunk_page: int
      :param approximation_depth: Used for approximate search to speed up search. The higher the number, faster the search but potentially less accurate.
      :type approximation_depth: int
      :param sum_fields: Whether to sum the multiple vectors similarity search score as 1 or seperate
      :type sum_fields: bool
      :param page_size: Size of each page of results
      :type page_size: int
      :param page: Page of the results
      :type page: int
      :param similarity_metric: Similarity Metric, choose from ['cosine', 'l1', 'l2', 'dp']
      :type similarity_metric: string
      :param facets: Fields to include in the facets, if [] then all
      :type facets: list
      :param filters: Query for filtering the search results
      :type filters: list
      :param min_score: Minimum score for similarity metric
      :type min_score: int
      :param include_vector: Include vectors in the search results
      :type include_vector: bool
      :param include_count: Include the total count of results in the search results
      :type include_count: bool
      :param asc: Whether to sort results by ascending or descending order
      :type asc: bool
      :param keep_search_history: Whether to store the history into VecDB. This will increase the storage costs over time.
      :type keep_search_history: bool
      :param hundred_scale: Whether to scale up the metric by 100
      :type hundred_scale: bool
      :param first_step_multivector_query: Query for advance search that allows for multiple vector and field querying.
      :type first_step_multivector_query: list
      :param first_step_page: Page of the results
      :type first_step_page: int
      :param first_step_page_size: Size of each page of results
      :type first_step_page_size: int
      :param query: What to store as the query name in the dashboard
      :type query: string


   .. py:method:: advanced_chunk(self, dataset_ids, chunk_search_query: List, min_score: int = None, page_size: int = 20, include_vector: bool = False, select_fields: list = [], query: str = None)

      A more advanced chunk search to be able to combine vector search and chunk search in many different ways.

      Example 1 (Hybrid chunk search):
      >>> chunk_query = {
      >>>     "chunk" : "some.test",
      >>>     "queries" : [
      >>>         {"vector" : vec1, "fields": {"some.test.some_chunkvector_":1},
      >>>         "traditional_query" : {"text":"python", "fields" : ["some.test.test_words"], "traditional_weight": 0.3},
      >>>         "metric" : "cosine"},
      >>>         {"vector" : vec, "fields": ["some.test.tt.some_other_chunkvector_"],
      >>>         "traditional_query" : {"text":"jumble", "fields" : ["some.test.test_words"], "traditional_weight": 0.3},
      >>>         "metric" : "cosine"},
      >>>     ]
      >>> }

      Example 2 (combines normal vector search with chunk search):
      >>> chunk_query = {
      >>>     "queries" : [
      >>>         {
      >>>             "queries": [
      >>>                 {
      >>>                     "vector": vec1,
      >>>                     "fields": {
      >>>                         "some.test.some_chunkvector_": 0.9
      >>>                     },
      >>>                     "traditional_query": {
      >>>                         "text": "python",
      >>>                         "fields": [
      >>>                             "some.test.test_words"
      >>>                         ],
      >>>                         "traditional_weight": 0.3
      >>>                     },
      >>>                     "metric": "cosine"
      >>>                 }
      >>>             ],
      >>>             "chunk": "some.test",
      >>>         },
      >>>         {
      >>>             "vector" : vec,
      >>>             "fields": {
      >>>                 ".some_vector_" : 0.1},
      >>>                 "metric" : "cosine"
      >>>                 },
      >>>         ]
      >>>     }

      :param dataset_id: Unique name of dataset
      :type dataset_id: string
      :param chunk_search_query: Advanced chunk query
      :type chunk_search_query: list
      :param min_score: Minimum score for similarity metric
      :type min_score: int
      :param page_size: Size of each page of results
      :type page_size: int
      :param include_vector: Include vectors in the search results
      :type include_vector: bool
      :param select_fields: Fields to include in the search results, empty array/list means all fields.
      :type select_fields: list
      :param query: What to store as the query name in the dashboard
      :type query: string


   .. py:method:: advanced_multistep_chunk(self, dataset_ids: list, first_step_query: list, first_step_text: str, first_step_fields: list, chunk_search_query: list, first_step_edit_distance: int = -1, first_step_ignore_space: bool = True, first_step_traditional_weight: float = 0.075, first_step_approximation_depth: int = 0, first_step_sum_fields: bool = True, first_step_filters: list = [], first_step_page_size: int = 50, include_count: bool = True, min_score: int = 0, page_size: int = 20, include_vector: bool = False, select_fields: list = [], query: str = None)

      Performs a vector hybrid search and then an advanced chunk search. Chunk Search allows one to search through chunks inside a document. The major difference between chunk search and normal search in Vector AI is that it relies on the chunkvector field. Chunk Vector Search. Search with a multiple chunkvectors for the most similar documents. Chunk search also supports filtering to only search through filtered results and facets to get the overview of products available when a minimum score is set.


      Example 1 (Hybrid chunk search):

      >>> chunk_query = {
      >>>     "chunk" : "some.test",
      >>>     "queries" : [
      >>>         {"vector" : vec1, "fields": {"some.test.some_chunkvector_":1},
      >>>         "traditional_query" : {"text":"python", "fields" : ["some.test.test_words"], "traditional_weight": 0.3},
      >>>         "metric" : "cosine"},
      >>>         {"vector" : vec, "fields": ["some.test.tt.some_other_chunkvector_"],
      >>>         "traditional_query" : {"text":"jumble", "fields" : ["some.test.test_words"], "traditional_weight": 0.3},
      >>>         "metric" : "cosine"},
      >>>     ]
      >>> }

      Example 2 (combines normal vector search with chunk search):
      >>> chunk_query = {
      >>>     "queries" : [
      >>>         {
      >>>             "queries": [
      >>>                 {
      >>>                     "vector": vec1,
      >>>                     "fields": {
      >>>                         "some.test.some_chunkvector_": 0.9
      >>>                     },
      >>>                     "traditional_query": {
      >>>                         "text": "python",
      >>>                         "fields": [
      >>>                             "some.test.test_words"
      >>>                         ],
      >>>                         "traditional_weight": 0.3
      >>>                     },
      >>>                     "metric": "cosine"
      >>>                 }
      >>>             ],
      >>>             "chunk": "some.test",
      >>>         },
      >>>         {
      >>>             "vector" : vec,
      >>>             "fields": {
      >>>                 ".some_vector_" : 0.1},
      >>>                 "metric" : "cosine"
      >>>                 },
      >>>         ]
      >>>     }

      :param dataset_id: Unique name of dataset
      :type dataset_id: string
      :param first_step_query: First step query
      :type first_step_query: list
      :param first_step_text: Text search query (not encoded as vector)
      :type first_step_text: string
      :param first_step_fields: Text fields to search against
      :type first_step_fields: list
      :param chunk_search_query: Advanced chunk query
      :type chunk_search_query: list
      :param first_step_edit_distance: This refers to the amount of letters it takes to reach from 1 string to another string. e.g. band vs bant is a 1 word edit distance. Use -1 if you would like this to be automated.
      :type first_step_edit_distance: int
      :param first_step_ignore_spaces: Whether to consider cases when there is a space in the word. E.g. Go Pro vs GoPro.
      :type first_step_ignore_spaces: bool
      :param first_step_traditional_weight: Multiplier of traditional search score. A value of 0.025~0.075 is the ideal range
      :type first_step_traditional_weight: int
      :param first_step_approximation_depth: Used for approximate search to speed up search. The higher the number, faster the search but potentially less accurate.
      :type first_step_approximation_depth: int
      :param first_step_sum_fields: Whether to sum the multiple vectors similarity search score as 1 or seperate
      :type first_step_sum_fields: bool
      :param first_step_filters: Query for filtering the search results
      :type first_step_filters: list
      :param first_step_page_size: In the first search, you are more interested in the contents
      :type first_step_page_size: int
      :param include_count: Include the total count of results in the search results
      :type include_count: bool
      :param min_score: Minimum score for similarity metric
      :type min_score: int
      :param page_size: Size of each page of results
      :type page_size: int
      :param include_vector: Include vectors in the search results
      :type include_vector: bool
      :param select_fields: Fields to include in the search results, empty array/list means all fields.
      :type select_fields: list
      :param query: What to store as the query name in the dashboard
      :type query: string


   .. py:method:: _init_experiment_helper(self, categories=['chunk', 'vector', 'diversity', 'traditional'])


   .. py:method:: make_suggestion(self)



