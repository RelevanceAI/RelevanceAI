|Open In Colab| # Quickstart to get features such as: - hybrid search -
multivector search - filtered search - etc out of the box

.. |Open In Colab| image:: https://colab.research.google.com/assets/colab-badge.svg
   :target: https://colab.research.google.com/github/RelevanceAI/RelevanceAI-readme-docs/blob/v2.0.0/docs/getting-started/example-applications/_notebooks/RelevanceAI-ReadMe-Question-Answering-using-USE-QA-Tensorflow-Hub.ipynb

What I Need
===========

-  Project & API Key (grab your API key from https://cloud.relevance.ai/
   in the settings area)
-  Python 3
-  Relevance AI Installed as shown below. You can also visit our
   `Installation guide <https://docs.relevance.ai/docs>`__

Installation Requirements
=========================

.. code:: ipython3

    # remove `!` if running the line in a terminal
    !pip install -U RelevanceAI[notebook]==2.0.0
    # remove `!` if running the line in a terminal
    !pip install vectorhub[encoders-text-tfhub]


Set up
======

To use Relevance AI, a client object must be instantiated. This needs an
API_key and a project name. These can be generated/access directly at
https://cloud.relevance.ai/ or simply by running the cell below and
following the link and the guide:

.. code:: ipython3

    from relevanceai import Client

    """
    You can sign up/login and find your credentials here: https://cloud.relevance.ai/sdk/api
    Once you have signed up, click on the value under `Activation token` and paste it here
    """
    client = Client()



Vector search
=============

1) Data
-------

For this quickstart we will be using a sample e-commerce dataset.
Alternatively, you can use your own dataset for the different steps.

.. code:: ipython3

    import pandas as pd
    from relevanceai.utils.datasets import get_ecommerce_dataset_clean

    # Retrieve our sample dataset. - This comes in the form of a list of documents.
    documents = get_ecommerce_dataset_clean()

    pd.DataFrame.from_dict(documents).head()


.. code:: ipython3

    documents[0].keys()

2) Encode
---------

.. code:: ipython3

    import tensorflow as tf
    import tensorflow_hub as hub
    import numpy as np
    import tensorflow_text

    # Here we load the model and define how we encode
    module = hub.load('https://tfhub.dev/google/universal-sentence-encoder-qa/3')

    # First we define how we encode the queries
    def encode_query(query: str):
        return module.signatures['question_encoder'](tf.constant([query]))['outputs'][0].numpy().tolist()

    # We then want to define how we encode the answers
    def encode_answer(answer: str):
        return module.signatures['response_encoder'](
            input=tf.constant([answer]),
            context=tf.constant([answer]))['outputs'][0].numpy().tolist()



.. code:: ipython3

    from tqdm.auto import tqdm

    for d in tqdm(documents):
        d['product_title_use_qa_vector_'] = encode_answer(d['product_title'])



3) Insert
---------

Uploading our documents into a dataset called ``quickstart_tfhub_qa``.
Note that each document should have a field called ’_id’. Here, we
generate a unique identifier using the Python uuid package.

.. code:: ipython3

    ds = client.Dataset("quickstart_tfhub_qa")
    ds.insert_documents(documents)


4) Search
---------

Note that our dataset includes vectors generated by Universal Sentence
Encoder. Therefore, in this step, we first vectorize the query using the
same encoder to be able to search among the similarly generated vectors.

.. code:: ipython3

    query = 'What is an expensive gift?'
    query_vector = encode_query(query)



.. code:: ipython3


    multivector_query=[
        { "vector": query_vector, "fields": ["product_title_use_qa_vector_"]}
    ]
    results = ds.vector_search(
        multivector_query=multivector_query,
        page_size=5
    )




.. parsed-literal::

    You can now visit the dashboard at https://cloud.relevance.ai/sdk/search


.. code:: ipython3

    from relevanceai import show_json

    print('=== QUERY === ')
    print(query)

    print('=== RESULTS ===')
    show_json(results, image_fields=["product_image"], text_fields=["product_title"])





.. raw:: html

    <table border="1" class="dataframe">
      <thead>
        <tr style="text-align: right;">
          <th></th>
          <th>product_image</th>
          <th>product_title</th>
          <th>_id</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <th>0</th>
          <td><img src="https://ak1.ostkcdn.com/images/products/9390080/P16579305.jpg" width="60" ></td>
          <td>Cienta Girl (Toddler) '108048' Basic Textile Boots</td>
          <td>97dae01b-3fdc-4555-8540-ca429098d94f</td>
        </tr>
        <tr>
          <th>1</th>
          <td><img src="https://ak1.ostkcdn.com/images/products/9635042/P16819991.jpg" width="60" ></td>
          <td>Nina Kids Girl (Toddler) 'Nesa' Man-Made Sandals</td>
          <td>7e29fab3-bcd9-456f-aa26-adac15ea3cd4</td>
        </tr>
        <tr>
          <th>2</th>
          <td><img src="https://ak1.ostkcdn.com/images/products/9908210/P17066979.jpg" width="60" ></td>
          <td>Evenflo Dottie Rose Convertible 3-in-1 High Chair</td>
          <td>40d1a884-a2b7-4cae-8e91-cb2c8350bc9f</td>
        </tr>
        <tr>
          <th>3</th>
          <td><img src="https://ak1.ostkcdn.com/images/products/8237318/P15565488.jpg" width="60" ></td>
          <td>Layla Chenille Bedspread (Shams Sold Separately)</td>
          <td>6b6e0ee6-a465-44f2-a6b3-6a545bfdc371</td>
        </tr>
        <tr>
          <th>4</th>
          <td><img src="https://ak1.ostkcdn.com/images/products/9472580/P16654939.jpg" width="60" ></td>
          <td>Osh Kosh Girl (Toddler) 'Rapid-14' Man-Made Sandals (Size 7 )</td>
          <td>147f7fae-e4e6-4f81-a7f2-d07f311c9ca0</td>
        </tr>
      </tbody>
    </table>
